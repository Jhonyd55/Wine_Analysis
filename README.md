# üç∑ Wine Quality EDA - An√°lisis Exploratorio del Dataset de Calidad del Vino

Este repositorio contiene un an√°lisis exploratorio de datos (EDA) realizado sobre el dataset de *Wine Quality (Red Wine)*, disponible en el [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wine+Quality). Este proyecto fue desarrollado con el objetivo de demostrar habilidades en an√°lisis de datos, visualizaci√≥n, y comunicaci√≥n de hallazgos.

> Tambi√©n puedes modificar f√°cilmente el c√≥digo para trabajar con el vino blanco o ambas muestras combinadas.

---

## üìÅ Contenido

- `wine_data_analysis.ipynb`: Notebook de Google Colab con el an√°lisis completo.
- `images/`: Carpeta con visualizaciones del an√°lisis (gr√°ficos generados desde el notebook).
- `data/`: Carpeta opcional para incluir el dataset si se desea.

---

## üîç Descripci√≥n del dataset

Los datasets contienen informaci√≥n sobre variantes tintas y blancas del vino portugu√©s "Vinho Verde". Incluyen variables fisicoqu√≠micas y una evaluaci√≥n sensorial de calidad (escala de 0 a 10). Se pueden abordar como problemas de regresi√≥n o clasificaci√≥n, aunque las clases no est√°n balanceadas. No se dispone de datos comerciales o de procedencia. Dada la naturaleza del dataset, es √∫til aplicar detecci√≥n de outliers y m√©todos de selecci√≥n de caracter√≠sticas.

**Caracter√≠sticas**:
- 1599 muestras de vino tinto.
- 11 variables f√≠sico-qu√≠micas como acidez, az√∫car residual, pH, alcohol, entre otros.
- Una variable objetivo: `quality`.

---
### üç∑ Flexibilidad en el an√°lisis: vino tinto, blanco o ambos

Este proyecto realiza un an√°lisis exploratorio sobre el dataset **Wine Quality** del repositorio UCI, enfocado por defecto en el **vino tinto** (*red wine*). No obstante, el c√≥digo ha sido dise√±ado de forma flexible para que puedas cambiar f√°cilmente el tipo de vino a analizar.

En la secci√≥n `üöÄ Cargar dataset` del notebook, puedes modificar el valor de la variable `variant` seg√∫n el an√°lisis deseado:

```python
# üöÄ Cargar dataset
wine_quality = load_wine_dataset()

# Selecciona:
# [1] para an√°lisis de vino tinto
# [2] para an√°lisis de vino blanco
# [3] para combinar ambos (tinto y blanco)
variant = 1  # Cambia este valor seg√∫n tu inter√©s
```
## üìä An√°lisis Exploratorio

El an√°lisis fue desarrollado siguiendo las buenas pr√°cticas de EDA e incluye:

### ‚úÖ Carga y limpieza de datos
- Revisi√≥n de tipos de datos, valores nulos y estad√≠sticas b√°sicas.

 ![estad√≠sticas b√°sicas](images/describe.png)

 Realizando un analisis de los datos. 
 
 ![datos nulos](images/info.png)

No hay datos faltantes y podemos decir que hay unos valores at√≠picos los cuales es bueno analizar.

### üîé Agrupaci√≥n de la calidad

Inicialmente, la variable `quality` presentaba una distribuci√≥n desequilibrada, con la mayor√≠a de los vinos concentrados en calificaciones intermedias. Esta situaci√≥n pod√≠a afectar negativamente el desempe√±o de los modelos de clasificaci√≥n, ya que una distribuci√≥n sesgada tiende a sesgar tambi√©n las predicciones del modelo hacia las clases mayoritarias.

Para abordar este problema y lograr un an√°lisis m√°s balanceado y robusto, se decidi√≥ **agrupar las calificaciones de `quality`** en dos categor√≠as principales:

- **Vino de baja calidad** (etiquetado como `1`)
- **Vino de alta calidad** (etiquetado como `2`)

Esta agrupaci√≥n no solo simplific√≥ el problema de clasificaci√≥n, sino que tambi√©n permiti√≥ mejorar el balance entre las clases, lo cual es fundamental para modelos m√°s precisos y generalizables.

A continuaci√≥n se presentan dos histogramas comparativos: el primero muestra la distribuci√≥n original de `quality`, y el segundo la distribuci√≥n despu√©s de la transformaci√≥n:

<table>
  <tr>
    <td align="center">
      <img src="images/quality_distribution.png" alt="Histograma de calidad del vino" width="400"/>
      <p><em>Antes de transformar <code>quality</code></em></p>
    </td>
    <td align="center">
      <img src="images/agrupados.png" alt="Despu√©s de agrupar quality" width="400"/>
      <p><em>Despu√©s de transformar <code>quality</code></em></p>
    </td>
  </tr>
</table>



#### ‚úÖ Tratamiento de outliers

Para identificar datos at√≠picos (outliers), se aplic√≥ el m√©todo del Z-modificado (Zmod) utilizando la mediana y la desviaci√≥n absoluta mediana (MAD). Este enfoque es m√°s robusto frente a distribuciones sesgadas o con valores extremos en comparaci√≥n con el Z-score cl√°sico.

Se detectaron valores at√≠picos en varias variables num√©ricas, y estos fueron revisados para su posterior tratamiento (eliminaci√≥n o an√°lisis por separado).
```python
def detectar_atipicos_zmod(df, umbral=3.5):
    outliers = {}

    for col in df.select_dtypes(include='number'):
        x = df[col]
        mediana = x.median()
        mad = np.median(np.abs(x - mediana))

        if mad == 0:
            continue  # Evitar divisi√≥n por cero

        zmod = 0.6745 * (x - mediana) / mad
        mask_outliers = np.abs(zmod) > umbral
        outliers[col] = df[mask_outliers][col]

    return outliers
```
 ![valores at√≠picos](images/atipicos.png)
 


 
### ‚úÖ An√°lisis univariado
Durante el an√°lisis univariado se examinaron las distribuciones individuales de cada variable f√≠sico-qu√≠mica presente en el dataset. A partir de estas gr√°ficas se observ√≥ que ciertas variables tienden a agruparse alrededor de valores espec√≠ficos para cada rango de quality, lo cual sugiere que es posible establecer diferencias entre vinos de distinta calidad bas√°ndose √∫nicamente en algunas de estas caracter√≠sticas.

Este comportamiento indica que variables como el alcohol, el pH, los sulfitos y la densidad podr√≠an ser √∫tiles para **diferenciar entre tipos de vino** y predecir su calidad de manera preliminar.

Entre las variables m√°s relevantes que mostraron patrones claros en su distribuci√≥n se encuentran:

<table>
  <tr>
    <td align="center">
      <img src="images/citicacidVSalcohol.png" alt="Citric Acid vs Alcohol" width="300"/>
      <p><em>Citric Acid vs Alcohol</em></p>
    </td>
    <td align="center">
      <img src="images/volatileacidityVSph.png" alt="Volatile Acidity vs pH" width="300"/>
      <p><em>Volatile Acidity vs pH</em></p>
    </td>
    <td align="center">
      <img src="images/volatileacidityVSsulphates.png" alt="Volatile Acidity vs Sulphates" width="300"/>
      <p><em>Volatile Acidity vs Sulphates</em></p>
    </td>
  </tr>
</table>

*Nota: A continuaci√≥n se muestran √∫nicamente algunas de las visualizaciones. Para acceder al conjunto completo de gr√°ficos, consulta el proyecto en Google Colab.*

Estas gr√°ficas permitieron identificar rangos y comportamientos t√≠picos de vinos de mayor o menor calidad, sentando las bases para un an√°lisis bivariado m√°s profundo y una futura etapa de modelado predictivo. 

### üß™ Ingenier√≠a de caracter√≠sticas
Se crearon nuevas variables derivadas a partir de las caracter√≠sticas originales, para explorar su impacto en la predicci√≥n de calidad del vino:

```python
# Nuevas caracter√≠sticas derivadas
df['alcohol_density'] = df['alcohol'] * df['density']
df['acidez_ratio'] = df['fixed_acidity'] / (df['volatile_acidity'] + 1e-5)
df['dilucion_efecto'] = df['residual_sugar'] / (df['density'] + 1e-5)
df['sulfur_ratio'] = df['total_sulfur_dioxide'] / (df['free_sulfur_dioxide'] + 1e-5)
df['score_equilibrio'] = (df['alcohol'] * df['sulphates']) / ((df['volatile_acidity'] + 1e-5) * (df['chlorides'] + 1e-5))
df['inv_density'] = 1 / (df['density'] + 1e-5)
```
Estas variables permiten capturar interacciones no lineales y relaciones m√°s complejas entre los compuestos qu√≠micos y la calidad del vino.  


### ‚úÖ An√°lisis bivariado

En esta etapa se exploraron las relaciones entre pares de variables para entender mejor c√≥mo interact√∫an entre s√≠ y c√≥mo estas interacciones se relacionan con la variable objetivo `quality`.

A partir del an√°lisis bivariado se detectaron correlaciones interesantes que confirman y complementan lo observado en el an√°lisis univariado. Las visualizaciones empleadas, como diagramas de dispersi√≥n y mapas de calor, permitieron identificar tendencias y patrones relevantes.

Algunas de las relaciones m√°s destacadas incluyen:

- **Alcohol vs Quality**: Existe una correlaci√≥n positiva clara, donde los vinos con mayor contenido de alcohol tienden a tener una mejor calidad sensorial.
- **Acidez vol√°til vs Quality**: Se observa una relaci√≥n negativa, indicando que niveles altos de acidez vol√°til est√°n asociados a vinos de menor calidad.
- **Sulphates vs Quality**: Se evidenci√≥ una ligera correlaci√≥n positiva, mostrando que los sulfitos pueden influir en la percepci√≥n del vino.
- **Density vs Alcohol**: Presentan una relaci√≥n inversa, lo cual puede ser √∫til al momento de seleccionar caracter√≠sticas para modelado.
- **Free Sulfur Dioxide vs Total Sulfur Dioxide**: Estas dos variables est√°n altamente correlacionadas, lo que puede llevar a considerar eliminar una de ellas en an√°lisis posteriores para evitar redundancia.

Adem√°s, se gener√≥ un **heatmap de correlaci√≥n** entre todas las variables num√©ricas, que sirvi√≥ como gu√≠a para detectar relaciones fuertes y posibles redundancias, as√≠ como para seleccionar variables clave que podr√≠an tener un mayor peso en un modelo predictivo.

Estas relaciones ser√°n fundamentales para los siguientes pasos del an√°lisis y permitir√°n construir modelos m√°s interpretables y precisos.


![Heatmap de correlaciones](images/mapacorrelacion.png)

## üîç An√°lisis de Correlaci√≥n - Mapa de Calor

A continuaci√≥n, se presenta el an√°lisis de correlaci√≥n utilizando un mapa de calor que permite visualizar la relaci√≥n entre las diferentes variables del dataset:

### üìà Conclusiones del Mapa de Correlaci√≥n

#### ‚úÖ Relaciones fuertes positivas

- **`fixed_acidity` y `citric_acid`** (**0.69**): Existe una fuerte relaci√≥n positiva. A mayor acidez fija, mayor es tambi√©n el contenido de √°cido c√≠trico.
- **`residual_sugar` y `dilucion_efecto`** (**1.00**): Relaci√≥n perfecta,ya que una es derivada de la otra. Se recomienda eliminar una para evitar multicolinealidad.
- **`free_sulfur_dioxide` y `total_sulfur_dioxide`** (**0.64**): El di√≥xido de azufre total incluye el libre, lo que justifica esta relaci√≥n.
- **`alcohol` y `inv_density`** (**0.55**) y relaci√≥n negativa con `density` (**-0.55**): El contenido de alcohol reduce la densidad del vino, lo cual es coherente.

#### ‚ùå Relaciones fuertes negativas

- **`fixed_acidity` y `pH`** (**-0.71**): A mayor acidez, menor pH. Relaci√≥n qu√≠mica esperada.
- **`acidez_ratio` y `volatile_acidity`** (**-0.81**): Cuando la acidez vol√°til aumenta, el ratio general disminuye, indicando vinos posiblemente de menor calidad.
- **`density` y `inv_density`** (**-1.00**): Relaci√≥n perfecta inversa. Son rec√≠procas matem√°ticamente.

#### üèÖ Relaci√≥n con la calidad (`quality`)

- **`alcohol`** (**0.44**): Es la variable m√°s correlacionada positivamente con la calidad. Vinos con mayor contenido alcoh√≥lico tienden a tener mejor puntuaci√≥n.
- **`volatile_acidity`** (**-0.39**): Alta acidez vol√°til tiende a reducir la calidad del vino.
- **`sulphates`** (**0.27**): Leve correlaci√≥n positiva. Puede contribuir a una mejor percepci√≥n del vino.

#### üìå Otras observaciones

- Las variables derivadas como `alcohol_density`, `score_equilibrio` y `acidez_ratio` muestran correlaciones relevantes con variables originales. Pueden ser √∫tiles, pero se debe revisar la **multicolinealidad** es decir eliminar las variables originales o estas.
- Algunas variables como `chlorides` y `residual_sugar` tienen correlaciones muy bajas con la calidad, lo que indica baja relevancia para modelos predictivos podemos omitirlas.

### üß† Recomendaciones

- **Eliminar variables duplicadas o altamente correlacionadas**, como `density`/`inv_density` o `residual_sugar`/`dilucion_efecto`.
- **Conservar variables clave** como `alcohol`, `volatile_acidity` y `sulphates`, dada su relevancia en la predicci√≥n de calidad.
- **Revisar la utilidad de variables derivadas**, evaluando si realmente aportan informaci√≥n adicional significativa.

---

Este an√°lisis proporciona una base s√≥lida para la selecci√≥n de caracter√≠sticas en futuras etapas de modelado predictivo. Por lo que para este analisis utilizaremos las siguientes variables.

```python
variables_select= [
    'alcohol',              # Alta correlaci√≥n positiva con quality (0.44)
    'sulphates',            # Correlaci√≥n moderada positiva con quality (0.27)
    'volatile_acidity',     # Correlaci√≥n negativa con quality (-0.39)
    'citric_acid',          # Correlaci√≥n leve positiva (0.09), pero relacionada con acidez
    'fixed_acidity',        # Correlaci√≥n leve positiva (0.12), √∫til junto con pH
    'pH',                   # Relacionada con acidez, aunque la correlaci√≥n directa es baja
    'alcohol_density',      # Derivada √∫til, correlaci√≥n positiva (0.48)
    'score_equilibrio',     # Derivada con correlaci√≥n (0.48), √∫til para modelos
    'acidez_ratio',         # Buena correlaci√≥n (0.44), relaci√≥n balanceada entre √°cidos
]
```
>Para realizar futuras pruebas puedes seleccionar otras variables para ver como influyen en el entrenamiento.

---

## üß™ Modelado 

El proyecto tambi√©n incluye una secci√≥n opcional donde se prepara el dataset para modelado (normalizaci√≥n, selecci√≥n de caracter√≠sticas), permitiendo probar algoritmos de regresi√≥n o clasificaci√≥n como regresi√≥n log√≠stica, √°rbol de decisi√≥n, etc.
### üîÑ Normalizaci√≥n

Dado que los datos presentan cierto sesgo en su distribuci√≥n y considerando que la clasificaci√≥n de un vino como "bueno" o "malo" se basa en criterios de cata que no est√°n completamente documentados, se tom√≥ la decisi√≥n de normalizar las variables seleccionadas para el entrenamiento y la prueba del modelo.

Para ello, se aplic√≥ la t√©cnica de Z-score, que permite escalar las variables en funci√≥n de su media y desviaci√≥n est√°ndar, asegurando que todas tengan una distribuci√≥n centrada y comparable. Esto mejora la estabilidad y el desempe√±o de los algoritmos de aprendizaje autom√°tico.

### üß™ Entrenamiento y Prueba
Los datos fueron divididos en dos subconjuntos: un 80% se destin√≥ al entrenamiento del modelo y el 20% restante se reserv√≥ para pruebas. Esta divisi√≥n se realiz√≥ de forma aleatoria y sin aplicar criterios adicionales, con el objetivo de evaluar el desempe√±o del modelo de manera objetiva sobre datos no vistos durante el entrenamiento.

## ü§ñ Comparaci√≥n de Modelos de Clasificaci√≥n

Para determinar qu√© algoritmo ofrece un mejor desempe√±o en la predicci√≥n de la calidad del vino, se entrenaron y evaluaron varios modelos de clasificaci√≥n. La m√©trica utilizada para la comparaci√≥n fue la **precisi√≥n (accuracy)**.

### üîç Modelos evaluados

- **Regresi√≥n Log√≠stica**
- **M√°quinas de Vectores de Soporte (SVM)**
- **√Årbol de Decisi√≥n**
- **Bosque Aleatorio (Random Forest)**
- **K-Nearest Neighbors (KNN)**

Cada modelo fue entrenado con los mismos datos normalizados y utilizando la misma divisi√≥n entre entrenamiento y prueba (80/20), sin aplicar t√©cnicas de ajuste fino (tuning) en esta etapa inicial.

### üìä Resultados obtenidos

| Modelo              | Precisi√≥n (Accuracy) |
|---------------------|----------------------|
| Regresi√≥n Log√≠stica | 0.7568               |
| SVM                 | 0.7645               |
| √Årbol de Decisi√≥n   | 0.7876               |
| Bosque Aleatorio    | **0.8070**           |
| KNN                 | 0.7568               |

> **Nota**: Se gener√≥ una advertencia (`FutureWarning`) durante la ejecuci√≥n con Bosque Aleatorio, relacionada con la concatenaci√≥n de DataFrames. Esta no afecta el c√°lculo del resultado.

### üèÜ Modelo seleccionado

El modelo con mejor precisi√≥n fue el **Bosque Aleatorio (Random Forest)**, alcanzando un valor de **0.8070**. Dado su rendimiento superior, se considera como el mejor candidato para una posterior etapa de ajuste de hiperpar√°metros y evaluaci√≥n final.

---
## üå≤ Configuraci√≥n del Modelo Random Forest

Para la etapa de clasificaci√≥n se seleccion√≥ el algoritmo **Random Forest**, debido a su capacidad de manejar conjuntos de datos complejos, evitar el sobreajuste y ofrecer buenos resultados sin requerir una gran cantidad de ajuste fino.

### üîß Hiperpar√°metros Seleccionados

Se eligieron los siguientes hiperpar√°metros para mejorar la precisi√≥n y robustez del modelo:

- **`n_estimators=200`**: Se utilizaron 200 √°rboles para aumentar la estabilidad de las predicciones. Un mayor n√∫mero de √°rboles tiende a reducir el error de generalizaci√≥n.

- **`criterion='gini'`**: Se emple√≥ el √≠ndice de Gini como medida de impureza. Es m√°s eficiente computacionalmente que la entrop√≠a, y en la mayor√≠a de los casos ofrece resultados similares.

- **`max_depth=20`**: Limitar la profundidad de los √°rboles a 20 ayuda a evitar el sobreajuste. Esto permite que los √°rboles capturen patrones complejos sin llegar a memorizar los datos de entrenamiento.

- **`min_samples_split=10`**: Se establece un umbral m√≠nimo de 10 muestras para dividir un nodo, lo cual obliga al √°rbol a generalizar m√°s y evita divisiones innecesarias por ruido.

- **`min_samples_leaf=1`**: Se permite como m√≠nimo una muestra por nodo hoja. Esto da flexibilidad al modelo para ajustar detalles finos sin ser demasiado restrictivo.

- **`max_features='sqrt'`**: Esta configuraci√≥n selecciona un subconjunto aleatorio de variables (ra√≠z cuadrada del total) para evaluar en cada divisi√≥n de nodo. Esta aleatoriedad **reduce la correlaci√≥n entre los √°rboles** del bosque, mejorando la generalizaci√≥n del modelo. Aunque se eliminaron los valores at√≠picos, esta estrategia sigue siendo √∫til para reducir la varianza del modelo.

- **`random_state=42`**: Se fija una semilla para asegurar que los resultados sean reproducibles.

- **`n_jobs=-1`**: Utiliza todos los n√∫cleos disponibles del procesador, acelerando el entrenamiento mediante paralelizaci√≥n.

---

## üìà Resultados Obtenidos

Tras entrenar el modelo con los par√°metros anteriores y evaluar sobre el conjunto de prueba (20% de los datos), se obtuvieron los siguientes resultados:

### ‚úÖ Precisi√≥n General del Modelo

**Accuracy (Precisi√≥n):** `0.80`  
Esto significa que el 80% de las predicciones coinciden con los valores reales de calidad del vino.

---

### üìä Reporte de Clasificaci√≥n

| Clase | Precisi√≥n | Recall | F1-Score | Soporte |
|-------|-----------|--------|----------|---------|
| 1     | 0.82      | 0.72   | 0.77     | 116     |
| 2     | 0.79      | 0.87   | 0.83     | 143     |

- La **clase 1** (vino de calidad inferior) presenta una mayor precisi√≥n, aunque menor recall, lo que indica que algunos casos no fueron detectados correctamente.
- La **clase 2** (vino de calidad superior) tiene mayor recall, es decir, se detecta la mayor√≠a de los vinos de buena calidad, aunque con menor precisi√≥n.

---

### üßÆ Matriz de Confusi√≥n

 ![Matriz de confusi√≥n 4](images/confusion.png)

- Se clasificaron correctamente 84 vinos de clase 1 y 124 vinos de clase 2.
- Hubo 32 vinos de clase 1 mal clasificados como clase 2, y 19 vinos de clase 2 mal clasificados como clase 1.

---

### üìù Conclusi√≥n

La configuraci√≥n del modelo Random Forest con los hiperpar√°metros seleccionados result√≥ efectiva para esta tarea de clasificaci√≥n binaria. Se obtuvo una precisi√≥n general del 80%, con buenos valores de f1-score para ambas clases. Se confirma que la eliminaci√≥n de valores at√≠picos, junto con la selecci√≥n cuidadosa de par√°metros como `max_depth`, `min_samples_split` y `max_features='sqrt'`, mejora la capacidad predictiva del modelo sin sobreajustarlo.


## üõ† Herramientas utilizadas

- Python 3.10+
- Google Colab
- Pandas
- Matplotlib / Seaborn
- Scikit-learn (para modelado opcional)

---

## üìÅ C√≥mo usar este proyecto

Puedes abrir directamente el notebook en Google Colab:

üìé [Ver en Google Colab](https://colab.research.google.com/drive/1lhzlRoNkSfiBZdrXBTcqROA7cgX-qd-K)

O clonarlo localmente:

```bash
git clone https://github.com/Jhonyd55/Wine_Analysis.git
```
## üôè Agradecimientos

Agradecemos profundamente el tiempo dedicado a revisar este an√°lisis. Esperamos que los resultados y conclusiones presentadas aqu√≠ sean de utilidad y sirvan como una gu√≠a clara y pr√°ctica para futuras investigaciones o an√°lisis relacionados.  

Este trabajo fue elaborado con el prop√≥sito de ofrecer una visi√≥n clara y fundamentada sobre el comportamiento de los vinos en funci√≥n de sus caracter√≠sticas qu√≠micas, apoy√°ndonos en t√©cnicas de ciencia de datos y modelos de aprendizaje autom√°tico.  

Quedamos a disposici√≥n para cualquier sugerencia o consulta adicional.

¬°Gracias por su atenci√≥n!

